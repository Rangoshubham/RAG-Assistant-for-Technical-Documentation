from langchain_google_genai import ChatGoogleGenerativeAI
import os
from dotenv import load_dotenv
from langchain_core.prompts import PromptTemplate
from langchain_community.document_loaders import PyMuPDFLoader
from langchain_google_genai import GoogleGenerativeAIEmbeddings
from langchain_text_splitters import RecursiveCharacterTextSplitter
from langchain_community.vectorstores import Chroma

import time
import hashlib
import json
import shutil

# Load environment variables from a .env file
load_dotenv()

# Get the API key from environment variables
google_api_key = os.getenv("GOOGLE_API_KEY")

# It's good practice to check if the key exists and raise an error if it doesn't.
if not google_api_key:
    raise ValueError("GOOGLE_API_KEY not found in environment variables.")

llm = ChatGoogleGenerativeAI(
    model="gemini-2.5-flash",
    google_api_key=google_api_key,
    temperature=0,
    max_tokens=None,
    timeout=None,
    max_retries=2,
)

def get_batch(iterable, batch_size):
    """Helper function to yield successive n-sized chunks from an iterable."""
    l = len(iterable)
    for ndx in range(0, l, batch_size):
        yield iterable[ndx:min(ndx + batch_size, l)]

def get_file_hash(filepath):
    """Calculates the MD5 hash of a file."""
    hasher = hashlib.md5()
    with open(filepath, 'rb') as f:
        buf = f.read()
        hasher.update(buf)
    return hasher.hexdigest()


def main():
    pdf_path = "attention.pdf"
    persist_directory = "./chroma_db"
    metadata_path = os.path.join(persist_directory, "metadata.json")
    
    vectordb = None
    
    # --- Check for existing vector store and document changes ---
    if os.path.exists(persist_directory):
        print("--- Found existing vector store. Checking for document changes... ---")
        
        current_pdf_hash = get_file_hash(pdf_path)
        saved_hash = ""

        if os.path.exists(metadata_path):
            with open(metadata_path, 'r') as f:
                metadata = json.load(f)
                saved_hash = metadata.get('hash')

        if current_pdf_hash == saved_hash:
            print("PDF has not changed. Loading existing vector store.\n")
            embeddings = GoogleGenerativeAIEmbeddings(model="models/gemini-embedding-001")
            vectordb = Chroma(persist_directory=persist_directory, embedding_function=embeddings)
        else:
            print("PDF has changed. Re-processing document.\n")
            shutil.rmtree(persist_directory) # Clean up old database

    if vectordb is None:
        print("--- Creating new vector store. ---")
        print("--- Loading PDF ---")
        loader = PyMuPDFLoader(pdf_path)
        docs = loader.load()
        print(f"PDF loaded into {len(docs)} documents (pages).\n")

        # --- 1. Split the document into chunks ---
        print("--- Splitting documents into chunks ---")
        text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=100)
        chunks = text_splitter.split_documents(docs)
        print(f"Split the {len(docs)} pages into {len(chunks)} chunks.\n")

        # --- 2. Create embeddings and store in ChromaDB ---
        print("--- Creating embeddings and storing in ChromaDB ---")
        embeddings = GoogleGenerativeAIEmbeddings(model="models/gemini-embedding-001")
        
        vectordb = Chroma.from_documents(
            documents=chunks, 
            embedding=embeddings,
            persist_directory=persist_directory
        )
        
        # Process chunks in batches to respect API rate limits
        batch_size = 50 # Process 50 chunks at a time
        for i, batch in enumerate(get_batch(chunks, batch_size)):
            print(f"Processing batch {i+1}/{ (len(chunks) // batch_size) + 1 }...")
            vectordb.add_documents(batch)
            # Add a delay between batches to avoid hitting rate limits
            # A 60-second delay for a batch of 50 is conservative and safe.
            print("Waiting for 60 seconds before next batch...")
            time.sleep(10)


        # Save metadata
        current_pdf_hash = get_file_hash(pdf_path)
        with open(metadata_path, 'w') as f:
            json.dump({'hash': current_pdf_hash}, f)
            
        print("Successfully created and saved the vector store.\n")

    embeddings = GoogleGenerativeAIEmbeddings(model="models/gemini-embedding-001")

    # --- 3. Ask a question and get an answer from the RAG system ---
    print("--- Ready to answer questions from the PDF ---")
    user_query = input("Please enter your question: ")

    if user_query and vectordb:
        print(f"\nSearching for relevant documents for: '{user_query}'")
        
        # Retrieve relevant chunks. The vector store handles embedding the query.
        retrieved_docs = vectordb.similarity_search(user_query, k=3)
        print(f"Found {len(retrieved_docs)} relevant document chunks.\n")

        context = "\n\n".join([doc.page_content for doc in retrieved_docs])

        template = """
You are a helpful assistant. Answer the question based only on the following context.
If you don't know the answer, just say that you don't know, don't try to make up an answer.

Context:
{context}

Question:
{question}

Answer:
"""
        prompt = template.format(context=context, question=user_query)
        
        print("--- Generating final answer ---")
        response = llm.invoke(prompt)
        print(f"\nAnswer:\n{response.content}")


if __name__ == "__main__":
    main()
